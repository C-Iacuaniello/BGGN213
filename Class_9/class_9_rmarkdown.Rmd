---
title: "Class_9"
author: "caroline iacuaniello"
date: "May 2, 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
url <- "https://bioboot.github.io/bggn213_S18/class-material/WisconsinCancer.csv"
```


# Complete the following code to input the data and store as wisc.df

```{r}
wisc.df <- read.csv(url)
```


# How many diagnosis are cancer vs non cancer?
```{r}
table(wisc.df$diagnosis)
```
# Lets make new data matric with just numeric values of interest, want to get rid of first 3 columns
# Convert the features of the data: wisc.data
```{r}
# wisc.data <- as.matrix()
# x[-c(1,4)]to subtract positions
# Note problem column "X" is 33, remove because throws error later!
wisc.data <- as.matrix(wisc.df[-c(1,2,33)])
rownames(wisc.data) <- wisc.df$id

```
# Finally, setup a separate new vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.
# Create diagnosis vector by completing the missing code
```{r}
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
diagnosis
sum(diagnosis)
```
# Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

#Q2. How many variables/features in the data are suffixed with _mean?

```{r}
ncol(wisc.data)
# grep("_mean", colnames(wisc.data), value = TRUE, invert = TRUE)
length(grep("mean", colnames(wisc.data)))

```

#Q3. How many of the observations have a malignant diagnosis?

```{r}
sum(diagnosis)
```

## Principle component analysis
# Check column means and standard deviations to determine if data should be scaled.
```{r}
colMeans(wisc.data)
plot(colMeans(wisc.data), type = "o")

apply(wisc.data,2,sd)
```
# Perform PCA on wisc.data by completing the following code
```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
# Look at summary of results (how well are we doing?)
summary(wisc.pr)

```
#Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44.27%
#Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
3
#Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data
5
###PCA results plots
Typically look at different inputs

```{r}
plot(wisc.pr$x[ ,1], wisc.pr$x[ ,2], col=diagnosis+1)
```
#Create a biplot of the wisc.pr using the biplot() function.
```{r}
biplot(wisc.pr)
```

#Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

## scree-plot: variance explained
# Variance explained by each principal component: pve
```{r}
pve <-  wisc.pr$sdev^2/sum(wisc.pr$sdev^2)
```


# Plot variance explained for each principal component

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
# scree not too good. lets try barplot
```{r}
barplot(pve, names.arg = paste("PC", 1:length(pve)), las=2, axes = FALSE, ylab="Proportion of Variance")
axis(2, at=pve, labels=round(pve,2)*100)
```


```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
# Plot cumulative proportion of variance explained



#Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature  concave.points_mean?
how original variables contribute to pcs?
#Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

###Section 3.
##Hierarchical clustering of case data
# Scale the wisc.data data: data.scaled
```{r}
data.scaled <- scale(wisc.data)

```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.

```{r}
data.dist <- dist(data.scaled)
```



Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to  wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist,"complete")
```


#Q11. Using the plot() function, what is the height at which the clustering model has 4 clusters? h=20

```{r}
plot(wisc.hclust)
abline(h=20, col="red", lwd=3)
```

Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

```
####How well do these groups match 'diagnosis'
```{r}
table(diagnosis)
```
```{r}
table(wisc.hclust.clusters)
```
```{r}
table(wisc.hclust.clusters, diagnosis)
```
# Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=8)
table(wisc.hclust.clusters, diagnosis)
```

Create a k-means model on wisc.data, assigning the result to wisc.km. Be sure to create 2 clusters, corresponding to the actual number of diagnosis. Also, remember to scale the data (with the scale() function and repeat the algorithm 20 times (by setting setting the value of the  nstart argument appropriately). Running multiple times such as this will help to find a well performing model.

```{r}
wisc.km <- kmeans(data.scaled , centers= 2, nstart=20)
```
```{r}
table(wisc.km$cluster)
table(wisc.km$cluster,diagnosis)
```
# Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
Cluster 1 has most cancer, but looks not clean. So far like hclust better-less false positives
```{r}
table(wisc.km$cluster,wisc.hclust.clusters)
```

##Section 5.
##Clustering on PCA results
Letâ€™s see if PCA improves or degrades the performance of hierarchical clustering.

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with complete linkage. Assign the results to wisc.pr.hclust.

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:3]), method="ward.D2")
plot(wisc.pr.hclust)
```
Cut into 4 groups
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=8)
table(wisc.pr.hclust.clusters,diagnosis)
```
Q14. How well does the newly created model with four clusters separate out the two diagnoses?
```{r}
plot(wisc.pr$x[ ,1:2], col=wisc.pr.hclust.clusters)
```
```{r}
# install.packages("rgl")
library(rgl)
```

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv((url))
npc <- predict(wisc.pr, newdata = new)
#plot(wisc.pr$x[ ,1:2], col=grps)
#points(npc[ ,1], npc[ ,2], col="blue", pch=16)
```

```{r}
plot(wisc.pr$x[ ,1:2], col=wisc.pr.hclust.clusters)
points(npc[ ,1], npc[ ,2], col=c("orange", "blue"), pch=16, cex=3)
```



